{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import telebot\n",
    "from config import TOKEN, LOG_FILE\n",
    "\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import scipy\n",
    "import pymystem3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "from keras.models import load_model\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from itertools import chain\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ENGINE_4(object):\n",
    "    def __init__(self, kbase_path, w2v_model):\n",
    "        self.knowledge_base = json.load(open(kbase_path))\n",
    "        self.lemmatizer = pymystem3.Mystem()\n",
    "        self.w2v_model = w2v_model\n",
    "        \n",
    "        # contains correct output for each class\n",
    "        self.answers = np.array([t['answer'] for t in self.knowledge_base])\n",
    "        self.tfidfmodel = self.prepare_vectorizer()\n",
    "        self.idf = self.tfidfmodel.idf_\n",
    "        self.idf_dict = {key:value for key, value in zip(self.tfidfmodel.get_feature_names(), self.idf)} \n",
    "        \n",
    "        self.vectorized_kbase, self.class_indexes = self.vectorize_knowledge_base()\n",
    "        \n",
    "    def prepare_vectorizer(self):\n",
    "        \"\"\"\n",
    "        Fits TF-IDF vectorizer using all available text from self.knowledge_base\n",
    "        \n",
    "        Returns TF-IDF vectorizer object\n",
    "        \"\"\"\n",
    "        # your code goes here\n",
    "        vectorizer = None\n",
    "        all_texts = []\n",
    "        \n",
    "        for kn in self.knowledge_base:\n",
    "            all_texts.append(kn['answer'])\n",
    "            for par in kn['paraphrased_questions']:\n",
    "                all_texts.append(par)\n",
    "            all_texts.append(kn['question'])\n",
    "        \n",
    "        # all_texts = [' '.join(self.tokenize_and_lemmatize(text)) for text in tqdm(texts)]\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(max_df = 0.9, min_df = 2, tokenizer=self.tokenize_and_lemmatize).fit(all_texts)\n",
    "\n",
    "        return vectorizer\n",
    "    \n",
    "    def vectorize(self, data):\n",
    "        \"\"\"\n",
    "        Turns a list of N strings into their vector representation using self.w2v_model.\n",
    "        In the simplest case, averages the word vectors of all words in a sentence.\n",
    "        Returns a a matrix of shape [N, 300]\n",
    "        \"\"\"\n",
    "        vectorized = []\n",
    "        # your code goes here\n",
    "        \n",
    "        for text in data:\n",
    "            zero_vector = np.zeros(self.w2v_model.vector_size)\n",
    "            word_vectors = []\n",
    "            \n",
    "            for token in self.tokenize_and_lemmatize(text):\n",
    "                try:\n",
    "                    word_vectors.append(self.w2v_model[token]*self.idf_dict[token])\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "            \n",
    "            if len(word_vectors):\n",
    "                vectorized.append(np.mean(word_vectors, axis=0))\n",
    "            else:\n",
    "                vectorized.append(zero_vector)\n",
    "        \n",
    "        return np.array(vectorized)\n",
    "        \n",
    "    def vectorize_knowledge_base(self):\n",
    "        \"\"\"\n",
    "        Vectorizes all questions AND paraphrased questions using the vectorize function.\n",
    "        Builds a list containing class id (it's index in self.knowledge_base) for each vectorized question.\n",
    "        \n",
    "        Example: you vectorized 1 question and 5 paraphrases for that question\n",
    "        Then you should append the ID of the question to class_labels list 5+1=6 times.\n",
    "        \"\"\"\n",
    "        vectors = []\n",
    "        class_labels = []\n",
    "        \n",
    "        # your code goes here\n",
    "        for num, kn in enumerate(self.knowledge_base):\n",
    "            step_kn = [kn['question']] + kn['paraphrased_questions']\n",
    "            vectors.append(self.vectorize(step_kn))\n",
    "            for i in range(len(step_kn)):\n",
    "                class_labels.append(num)\n",
    "            \n",
    "        return np.vstack(vectors), np.array(class_labels)\n",
    "    \n",
    "    def compute_class_scores(self, similarities):\n",
    "        \"\"\"\n",
    "        Accepts an array of similarities of shape (self.class_indexes, )\n",
    "        Computes scores for classes.\n",
    "        Returns a dictionary of size (n_classes) that looks like\n",
    "        {\n",
    "            0: 0.3,\n",
    "            1: 0.1,\n",
    "            2: 0.0,\n",
    "            class_n_id: class_n_score\n",
    "            ...\n",
    "        }\n",
    "        \"\"\"\n",
    "        \n",
    "        class_scores = dict(zip(range(len(self.answers)), [0]*len(self.answers)))\n",
    "        \n",
    "        for n,i in enumerate(self.class_indexes):\n",
    "            class_scores[i] += similarities[n]\n",
    "        \n",
    "        # your code goes here\n",
    "        return class_scores\n",
    "        \n",
    "    def tokenize_and_lemmatize(self, text):\n",
    "        analysis = self.lemmatizer.analyze(text.strip())\n",
    "        tokens = []\n",
    "        for an in analysis:\n",
    "            if 'analysis' in an:\n",
    "                try:\n",
    "                    tokens.append(an['analysis'][0]['lex'])\n",
    "                except IndexError:\n",
    "                    tokens.append(an['text'])\n",
    "        return tokens\n",
    "    \n",
    "    def get_top(self, query, top_k=3):\n",
    "        if isinstance(query, str):\n",
    "            query = [query]\n",
    "            \n",
    "        vectorized_query = self.vectorize(query)\n",
    "        css = cosine_similarity(vectorized_query, self.vectorized_kbase)[0]\n",
    "        scores = self.compute_class_scores(css)\n",
    "        \n",
    "        sorted_scores = sorted(scores.items(), key= lambda x: x[1])[::-1][:top_k]\n",
    "        top_classes = np.array([c[0] for c in sorted_scores])\n",
    "        top_answers = list(self.answers[top_classes])\n",
    "        return top_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-6abd04ec1b15>:46: SyntaxWarning: name 'graph' is assigned to before global declaration\n",
      "  global graph\n"
     ]
    }
   ],
   "source": [
    "class Backend(object):\n",
    "    \"\"\"\n",
    "    class contets few machine learning text handle methods\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, w2v_path, engine_w2v_path, json_path):\n",
    "        \"\"\"\n",
    "        initialize our class with model file, w2v file and json faq files \n",
    "        \"\"\"\n",
    "        self.model = load_model(model_path)\n",
    "        self.w2v = Word2Vec.load(w2v_path)\n",
    "        self.w2v_engine = KeyedVectors.load_word2vec_format(engine_w2v_path)\n",
    "        self.faq = json_path\n",
    "        self.tokenizer = RegexpTokenizer('[a-zA-Z0-9@]+')\n",
    "        self.stemmer = LancasterStemmer()\n",
    "        self.graph = tf.get_default_graph()\n",
    "        self.engine = ENGINE_4(self.json_path, self.w2v_engine)\n",
    "        \n",
    "    def preproc_tweet(self, text):\n",
    "        \"\"\"\n",
    "        function preprocesing tweet to good format\n",
    "        \"\"\"\n",
    "        max_tweet_length = 20\n",
    "        vector_size = 512\n",
    "        \n",
    "        matrix = np.zeros((1, max_tweet_length, vector_size), dtype=K.floatx())\n",
    "\n",
    "        for t, token in enumerate(self.tokenizer.tokenize(text.lower())):\n",
    "            if t >= max_tweet_length:\n",
    "                break\n",
    "            try:    \n",
    "                matrix[0, t, :] = self.w2v[self.stemmer.stem(token)]\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        return matrix\n",
    "    \n",
    "    def predict_sentiment(self, matrix):\n",
    "        \"\"\"\n",
    "        fucntion returns sentiment\n",
    "        argeuments:\n",
    "            matrix - numpy array, matrix of w2v words ebeddings\n",
    "            model - keras deep learning model\n",
    "        \"\"\"\n",
    "        graph = self.graph\n",
    "        global graph\n",
    "        with graph.as_default():\n",
    "            sentiment = self.model.predict(matrix)[0][1]\n",
    "        \n",
    "        return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepr = bk.predict_sentiment(bk.preproc_tweet('What a lonely day! And it\\'s mine'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bot = telebot.TeleBot(TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimension (-1) must be in the range [0, 2), where 2 is the number of dimensions in the input. for 'metrics/acc/ArgMax' (op: 'ArgMax') with input shapes: [?,?], [].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/p35/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    669\u001b[0m           \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors_as_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m           status)\n\u001b[0m\u001b[1;32m    671\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p35/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p35/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    468\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    470\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimension (-1) must be in the range [0, 2), where 2 is the number of dimensions in the input. for 'metrics/acc/ArgMax' (op: 'ArgMax') with input shapes: [?,?], [].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-147b7e1172af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load some backend for bot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weights-improvement-09-0.81.hdf5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test.wv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'word_vectors.w2v'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'./new_faq.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFormatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%(asctime)s - %(name)s - %(levelname)s - %(message)s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEBUG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-6abd04ec1b15>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, w2v_path, engine_w2v_path, json_path)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0minitialize\u001b[0m \u001b[0mour\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2v\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mjson\u001b[0m \u001b[0mfaq\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \"\"\"\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2v_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine_w2v_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p35/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    269\u001b[0m                       \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                       \u001b[0mloss_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m                       sample_weight_mode=sample_weight_mode)\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;31m# Set optimizer weights.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p35/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    819\u001b[0m                            \u001b[0mweighted_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweighted_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m                            \u001b[0mtarget_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                            **kwargs)\n\u001b[0m\u001b[1;32m    822\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p35/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    932\u001b[0m                         \u001b[0mappend_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mhandle_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m                 \u001b[0mhandle_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_weighted_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p35/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mhandle_metrics\u001b[0;34m(metrics, weights)\u001b[0m\n\u001b[1;32m    929\u001b[0m                             metric_result = weighted_metric_fn(y_true, y_pred,\n\u001b[1;32m    930\u001b[0m                                                                \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                                                                mask=masks[i])\n\u001b[0m\u001b[1;32m    932\u001b[0m                         \u001b[0mappend_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p35/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mweighted\u001b[0;34m(y_true, y_pred, weights, mask)\u001b[0m\n\u001b[1;32m    457\u001b[0m         \"\"\"\n\u001b[1;32m    458\u001b[0m         \u001b[0;31m# score_array has ndim >= 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m         \u001b[0mscore_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0;31m# Cast the mask to floatX to avoid float64 upcasting in theano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p35/lib/python3.5/site-packages/keras/metrics.py\u001b[0m in \u001b[0;36mcategorical_accuracy\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcategorical_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     return K.cast(K.equal(K.argmax(y_true, axis=-1),\n\u001b[0m\u001b[1;32m     26\u001b[0m                           K.argmax(y_pred, axis=-1)),\n\u001b[1;32m     27\u001b[0m                   K.floatx())\n",
      "\u001b[0;32m~/miniconda3/envs/p35/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(x, axis)\u001b[0m\n\u001b[1;32m   1385\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \"\"\"\n\u001b[0;32m-> 1387\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p35/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(input, axis, name, dimension)\u001b[0m\n\u001b[1;32m    247\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot specify both 'axis' and 'dimension'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p35/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36marg_max\u001b[0;34m(input, dimension, name)\u001b[0m\n\u001b[1;32m    166\u001b[0m   \"\"\"\n\u001b[1;32m    167\u001b[0m   result = _op_def_lib.apply_op(\"ArgMax\", input=input, dimension=dimension,\n\u001b[0;32m--> 168\u001b[0;31m                                 name=name)\n\u001b[0m\u001b[1;32m    169\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p35/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    757\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    758\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    760\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2240\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2242\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2243\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1615\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1617\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1618\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m~/miniconda3/envs/p35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p35/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p35/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimension (-1) must be in the range [0, 2), where 2 is the number of dimensions in the input. for 'metrics/acc/ArgMax' (op: 'ArgMax') with input shapes: [?,?], []."
     ]
    }
   ],
   "source": [
    "# load some backend for bot\n",
    "bk = Backend('../lecture1/weights-improvement-09-0.81.hdf5', '../lecture1/test.wv','word_vectors.w2v','./new_faq.json')\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# Logging to file\n",
    "fh = logging.FileHandler(LOG_FILE)\n",
    "fh.setLevel(logging.DEBUG)\n",
    "fh.setFormatter(formatter)\n",
    "# Logging to console\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "ch.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(fh)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "@bot.message_handler(commands=['start', 'help'])\n",
    "def greetings(message):\n",
    "    try:\n",
    "        logger.info('Started chat # {}'.format(message.chat.id))\n",
    "        bot.reply_to(message, 'Приветики, {} {}! Меня зовут {} (если коротко, то Сэнди), я к твоим услугам.\\n'\n",
    "                    'Вот что я могу:\\n'\n",
    "                    '/start - Начинаю чат и пишу тебе все, что умею (каждый раз, чтобы не забыл)\\n'\n",
    "                    '/help - Печатаю помощь (да, чтобы не забыл ;))\\n'\n",
    "                    '/sentiment [message] - предсказываю с помощью хрустального шара положительную (1) или отрицательную (0) окраску твита (только по-аглийски)\\n'\n",
    "                    '/answer [message] - Отвечаю по моему справочнику FAQ (надеюсь, что это не что-то неприличное) на твое сообщение'.format(message.chat.first_name,\n",
    "                                                                        message.chat.last_name, bot.get_me().username))\n",
    "    except:\n",
    "        bot.send_message(message.chat.id, 'Извини, я ошиблась. С кем не бывает?!')\n",
    "        \n",
    "@bot.message_handler(commands=['sentiment'])\n",
    "def predict_sentiment(message):\n",
    "    try:\n",
    "        text = ' '.join(message.text.split(' ')[1:]) # some idiot bicycle to delete command from text\n",
    "        result = bk.predict_sentiment(bk.preproc_tweet(text))\n",
    "        bot.send_message(message.chat.id, result)\n",
    "        logger.info('Predicted sentiment {} for tweet {}'.format(result, text))\n",
    "    except:\n",
    "        bot.send_message(message.chat.id, 'Извини, я ошиблась. С кем не бывает?!')\n",
    "        \n",
    "@bot.message_handler(commands=['answer'])\n",
    "def answer(message):\n",
    "    answer = bk.engine.get_top(message, top_k=1)\n",
    "    bot.send_message(message.chat.id, answer[0])\n",
    "    \n",
    "@bot.message_handler(content_types=['text'])\n",
    "def small_talk(message):\n",
    "    bot.send_message(message.chat.id, \"Я бы с тобой поговорила, да вот сказать нечего :(\\n Введи-ка лучше команду.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-03 13:25:35,663 - __main__ - INFO - Started chat # 74141678\n",
      "2017-12-03 13:27:08,034 - __main__ - INFO - Predicted sentiment 0.8989554047584534 for tweet We will rise again\n",
      "2017-12-03 13:27:24,596 - __main__ - INFO - Predicted sentiment 0.009651520289480686 for tweet I am so sick\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    bot.polling(none_stop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
